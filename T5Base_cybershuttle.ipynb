{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33cc16f",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bcdac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --no-cache-dir --force-reinstall airavata-python-sdk[notebook]\n",
    "import airavata_jupyter_magic\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC1:cloud,expanse:shared,anvil:shared\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539be9bd",
   "metadata": {},
   "source": [
    "## 2. The Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516a652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer, MarianMTModel, MarianTokenizer\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================== TRANSLATE FUNCTION ===========================================\n",
    "# Function to get the model name based on the source and target language\n",
    "def get_model_name(source_language, target_language):\n",
    "    return f\"Helsinki-NLP/opus-mt-{source_language}-{target_language}\"\n",
    "\n",
    "# Function to perform translation\n",
    "def translate_text(input_text, source_language='en', target_language='es'):\n",
    "    model_name = get_model_name(source_language, target_language)\n",
    "    \n",
    "    # Load the MarianMT model and tokenizer for the specific language pair\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare the input text with the correct prefix for translation\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Generate the translation\n",
    "    translated_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the translated output\n",
    "    translation = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# =========================================== SUMMARIZE FUNCTION ===========================================\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize_text(input_text):\n",
    "    # Preprocess input\n",
    "    input_text = input_text.strip()\n",
    "    if len(input_text.split()) < 15:  # Minimum words needed for good summary\n",
    "        return \"Input too short - please provide at least 15-20 words for meaningful summarization.\"\n",
    "    \n",
    "    # Format for T5 (crucial!)\n",
    "    input_text = \"summarize: \" + input_text\n",
    "    \n",
    "    # Tokenize with better truncation\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"  # Helps with short texts\n",
    "    )\n",
    "    \n",
    "    # Generate with adjusted parameters\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,       # Reduced from 150\n",
    "        min_length=30,        # Reduced from 50\n",
    "        length_penalty=3.0,   # Increased to favor shorter summaries\n",
    "        num_beams=6,          # Increased from 4\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3  # Prevents word repetition\n",
    "    )\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process output\n",
    "    if summary.lower() == input_text[11:].lower():  # If output == input\n",
    "        return \"Summary failed (input may be too short or unclear). Try with longer text.\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# =========================================== Q&A FUNCTION ===========================================\n",
    "def answer_question(context, question):\n",
    "    \"\"\"\n",
    "    Enhanced question answering with T5\n",
    "    Args:\n",
    "        context: Background information text (1-3 sentences work best)\n",
    "        question: Clear question about the context\n",
    "    Returns:\n",
    "        Concise answer extracted from context\n",
    "    \"\"\"\n",
    "    # Improved input formatting\n",
    "    input_text = f\"answer question based on context: {question} context: {context}\"\n",
    "    \n",
    "    # Better tokenization with attention to question-context balance\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"  # Helps with consistency\n",
    "    )\n",
    "    \n",
    "    # Optimized generation parameters\n",
    "    answer_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,        # More concise answers\n",
    "        min_length=5,          # Avoid empty answers\n",
    "        num_beams=5,           # Better quality than 4 beams\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=2.5, # Reduce repeated phrases\n",
    "        length_penalty=1.5,     # Prefer shorter answers\n",
    "        no_repeat_ngram_size=3, # Prevent word repetition\n",
    "        temperature=0.7         # Adds slight creativity\n",
    "    )\n",
    "    \n",
    "    # Improved decoding\n",
    "    answer = tokenizer.decode(\n",
    "        answer_ids[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    # Post-processing for better results\n",
    "    answer = answer.split(\".\")[0]  # Take the first complete thought\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    return answer if answer else \"I couldn't find an answer in the context.\"\n",
    "\n",
    "# =========================================== CLASSIFY FUNCTION ===========================================\n",
    "hf_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(input_text):\n",
    "    \"\"\"\n",
    "    Hybrid sentiment analysis combining transformer models with VADER intensity analysis\n",
    "    Returns formatted string with nuanced sentiment assessment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get HuggingFace predictions\n",
    "        hf_results = hf_classifier(input_text, truncation=True)[0]\n",
    "        pos_score = next(r['score'] for r in hf_results if r['label'] == 'POS')\n",
    "        neg_score = next(r['score'] for r in hf_results if r['label'] == 'NEG')\n",
    "        \n",
    "        # Get VADER intensity scores\n",
    "        vader_scores = vader.polarity_scores(input_text)\n",
    "        \n",
    "        # Combined weighted score (70% HF, 30% VADER)\n",
    "        combined_pos = (pos_score * 0.7) + (vader_scores['pos'] * 0.3)\n",
    "        combined_neg = (neg_score * 0.7) + (vader_scores['neg'] * 0.3)\n",
    "        \n",
    "        # Determine final sentiment\n",
    "        if combined_pos > combined_neg:\n",
    "            sentiment = \"POSITIVE\"\n",
    "            base_confidence = combined_pos\n",
    "            intensity = vader_scores['pos']\n",
    "        else:\n",
    "            sentiment = \"NEGATIVE\"\n",
    "            base_confidence = combined_neg\n",
    "            intensity = vader_scores['neg']\n",
    "        \n",
    "        # Dynamic confidence adjustment based on intensity\n",
    "        adjusted_confidence = min(base_confidence * (1 + intensity), 0.99)\n",
    "        \n",
    "        # Strength classification with wider bands\n",
    "        strength_ranges = [\n",
    "            (0.9, \"Extremely\"),\n",
    "            (0.8, \"Very\"),\n",
    "            (0.7, \"Strongly\"),\n",
    "            (0.6, \"Fairly\"),\n",
    "            (0.5, \"Moderately\"),\n",
    "            (0.4, \"Somewhat\"),\n",
    "            (0, \"Slightly\")\n",
    "        ]\n",
    "        \n",
    "        strength = next(\n",
    "            desc for threshold, desc in strength_ranges \n",
    "            if adjusted_confidence >= threshold\n",
    "        )\n",
    "        \n",
    "        # Add intensity qualifiers\n",
    "        modifiers = {\n",
    "            \"Extremely\": \"!\",\n",
    "            \"Very\": \"!\",\n",
    "            \"Strongly\": \"\",\n",
    "            \"Fairly\": \"\",\n",
    "            \"Moderately\": \" (somewhat)\",\n",
    "            \"Somewhat\": \" (mildly)\",\n",
    "            \"Slightly\": \" (barely)\"\n",
    "        }\n",
    "        \n",
    "        return (\n",
    "            f\"{strength} {sentiment}{modifiers[strength]} \"\n",
    "            f\"(Confidence: {adjusted_confidence:.0%})\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Analysis error: {str(e)}\"\n",
    "\n",
    "# =========================================== MAIN FUNCTION ===========================================\n",
    "# Language mapping dictionary\n",
    "LANGUAGE_MAP = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Español (Spanish)\",\n",
    "    \"fr\": \"Français (French)\",\n",
    "    \"de\": \"Deutsch (German)\",\n",
    "    \"it\": \"Italiano (Italian)\",\n",
    "    \"pt\": \"Português (Portuguese)\",\n",
    "    \"ja\": \"日本語 (Japanese)\",\n",
    "    \"zh\": \"中文 (Chinese)\"\n",
    "}\n",
    "\n",
    "language_options = [f\"{code} - {name}\" for code, name in LANGUAGE_MAP.items()]\n",
    "\n",
    "def validate_input(task, input_text):\n",
    "    if not input_text.strip():\n",
    "        raise gr.Error(\"Input text cannot be empty!\")\n",
    "\n",
    "    if task == \"Answer Question\":\n",
    "        lines = input_text.strip().split(\"\\n\")\n",
    "        if len(lines) < 2:\n",
    "            raise gr.Error(\n",
    "                \"For 'Answer Question', input must have:\\n\"\n",
    "                \"Line 1: Context (text with the answer)\\n\"\n",
    "                \"Line 2: Question\"\n",
    "            )\n",
    "    return True\n",
    "\n",
    "def run_task(task, input_text, source_lang, target_lang):\n",
    "    validate_input(task, input_text)\n",
    "\n",
    "    if task == \"Summarize\":\n",
    "        return summarize_text(input_text)\n",
    "    elif task == \"Translate\":\n",
    "        source_code = source_lang.split(\" \")[0]\n",
    "        target_code = target_lang.split(\" \")[0]\n",
    "        return translate_text(input_text, source_code, target_code)\n",
    "    elif task == \"Answer Question\":\n",
    "        lines = input_text.strip().split(\"\\n\")\n",
    "        context, question = lines[0], lines[1]\n",
    "        return answer_question(context, question)\n",
    "    elif task == \"Classify\":\n",
    "        return classify_sentiment(input_text)\n",
    "    else:\n",
    "        return \"Please select a valid task.\"\n",
    "\n",
    "def show_translation_fields(task):\n",
    "    return gr.update(visible=(task == \"Translate\"))\n",
    "\n",
    "def update_help_text(task):\n",
    "    if task == \"Answer Question\":\n",
    "        return \"Tip: Enter context (line 1) and question (line 2).\"\n",
    "    elif task == \"Translate\":\n",
    "        return \"Tip: Enter text and select source and target languages.\"\n",
    "    else:\n",
    "        return \"Tip: Enter text and click 'Run Task'.\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🧠 AI NLP Tool\")\n",
    "\n",
    "    task = gr.Dropdown([\"Summarize\", \"Translate\", \"Answer Question\", \"Classify\"], label=\"Select Task\", value=\"Summarize\")\n",
    "    help_message = gr.Markdown(\"Tip: Enter text and click 'Run Task'.\")\n",
    "\n",
    "    input_text = gr.Textbox(label=\"Enter Input Text\", lines=8, placeholder=\"Enter your text here...\")\n",
    "    \n",
    "    with gr.Row(visible=False) as language_row:\n",
    "        source_lang = gr.Dropdown(language_options, label=\"From\", value=language_options[0])\n",
    "        target_lang = gr.Dropdown(language_options, label=\"To\", value=language_options[1])\n",
    "\n",
    "    run_btn = gr.Button(\"Run Task\")\n",
    "    output_text = gr.Textbox(label=\"Result\", lines=8)\n",
    "\n",
    "    # Interactions\n",
    "    task.change(fn=show_translation_fields, inputs=task, outputs=language_row)\n",
    "    task.change(fn=update_help_text, inputs=task, outputs=help_message)\n",
    "    run_btn.click(fn=run_task, inputs=[task, input_text, source_lang, target_lang], outputs=output_text)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
